#!/usr/bin/env python3
"""
Google Search Console ìƒ‰ì¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸

Google Indexing APIë¥¼ ì‚¬ìš©í•˜ì—¬ URL ìƒ‰ì¸ì„ ìë™ìœ¼ë¡œ ìš”ì²­í•©ë‹ˆë‹¤.
ì¼ì¼ 200ê°œ ì œí•œì„ ê³ ë ¤í•˜ì—¬ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python indexing_automation.py

ìš”êµ¬ì‚¬í•­:
    - Google Cloud Consoleì—ì„œ Indexing API í™œì„±í™”
    - ì„œë¹„ìŠ¤ ê³„ì • JSON í‚¤ íŒŒì¼ (credentials.json)
    - pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client

"""

import os
import json
import csv
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging

# Google API í´ë¼ì´ì–¸íŠ¸
try:
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
    GOOGLE_API_AVAILABLE = True
except ImportError:
    GOOGLE_API_AVAILABLE = False
    print("âŒ Google API í´ë¼ì´ì–¸íŠ¸ ë¯¸ì„¤ì¹˜")
    print("   pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client")
    exit(1)


# ì„¤ì •
CREDENTIALS_FILE = 'credentials.json'  # Google ì„œë¹„ìŠ¤ ê³„ì • JSON í‚¤
SCOPES = ['https://www.googleapis.com/auth/indexing']
URL_LIST_FILE = 'url_priority_list.csv'
LOG_FILE = 'indexing_log.json'
DAILY_LIMIT = 200  # Google Indexing API ì¼ì¼ ì œí•œ
BATCH_SIZE = 10    # í•œ ë²ˆì— ì²˜ë¦¬í•  URL ìˆ˜

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('indexing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class IndexingAutomation:
    """Google Indexing API ìë™í™” í´ë˜ìŠ¤"""

    def __init__(self, credentials_file: str = CREDENTIALS_FILE):
        self.credentials_file = credentials_file
        self.service = None
        self.log_data = self.load_log()

        if not os.path.exists(self.credentials_file):
            logger.error(f"âŒ ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.credentials_file}")
            logger.error("   Google Cloud Consoleì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • JSON í‚¤ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")
            raise FileNotFoundError(self.credentials_file)

        self.authenticate()

    def authenticate(self):
        """Google API ì¸ì¦"""
        try:
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_file,
                scopes=SCOPES
            )
            self.service = build('indexing', 'v3', credentials=credentials)
            logger.info("âœ… Google Indexing API ì¸ì¦ ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ ì¸ì¦ ì‹¤íŒ¨: {e}")
            raise

    def load_log(self) -> Dict:
        """ë¡œê·¸ íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(LOG_FILE):
            with open(LOG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            'last_run': None,
            'total_submitted': 0,
            'success_count': 0,
            'error_count': 0,
            'urls': {}
        }

    def save_log(self):
        """ë¡œê·¸ íŒŒì¼ ì €ì¥"""
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.log_data, f, ensure_ascii=False, indent=2)

    def load_urls(self) -> List[Dict]:
        """URL ëª©ë¡ ë¡œë“œ"""
        if not os.path.exists(URL_LIST_FILE):
            logger.error(f"âŒ URL ëª©ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {URL_LIST_FILE}")
            logger.error("   ë¨¼ì € url_priority_generator.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
            return []

        urls = []
        with open(URL_LIST_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                urls.append(row)

        logger.info(f"ğŸ“‹ URL ëª©ë¡ ë¡œë“œ: {len(urls)}ê°œ")
        return urls

    def check_daily_limit(self) -> bool:
        """ì¼ì¼ ì œí•œ í™•ì¸"""
        today = datetime.now().strftime('%Y-%m-%d')
        last_run = self.log_data.get('last_run')

        if last_run and last_run.startswith(today):
            submitted_today = self.log_data.get('submitted_today', 0)
            if submitted_today >= DAILY_LIMIT:
                logger.warning(f"âš ï¸  ì¼ì¼ ì œí•œ ë„ë‹¬: {submitted_today}/{DAILY_LIMIT}")
                return False
        else:
            # ìƒˆë¡œìš´ ë‚ ì§œ, ì¹´ìš´í„° ë¦¬ì…‹
            self.log_data['submitted_today'] = 0

        return True

    def request_indexing(self, url: str) -> bool:
        """ë‹¨ì¼ URL ìƒ‰ì¸ ìš”ì²­"""
        if not self.service:
            logger.error("âŒ API ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False

        try:
            body = {
                'url': url,
                'type': 'URL_UPDATED'  # URL_UPDATED ë˜ëŠ” URL_DELETED
            }

            response = self.service.urlNotifications().publish(body=body).execute()
            logger.info(f"âœ… ìƒ‰ì¸ ìš”ì²­ ì„±ê³µ: {url}")

            # ë¡œê·¸ ì—…ë°ì´íŠ¸
            self.log_data['urls'][url] = {
                'status': 'submitted',
                'timestamp': datetime.now().isoformat(),
                'response': response
            }
            self.log_data['success_count'] += 1
            self.log_data['submitted_today'] = self.log_data.get('submitted_today', 0) + 1

            return True

        except HttpError as e:
            error_msg = str(e)
            logger.error(f"âŒ ìƒ‰ì¸ ìš”ì²­ ì‹¤íŒ¨: {url} - {error_msg}")

            # ë¡œê·¸ ì—…ë°ì´íŠ¸
            self.log_data['urls'][url] = {
                'status': 'error',
                'timestamp': datetime.now().isoformat(),
                'error': error_msg
            }
            self.log_data['error_count'] += 1

            return False

        except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ: {url} - {e}")
            return False

    def process_batch(self, urls: List[Dict], batch_size: int = BATCH_SIZE):
        """ë°°ì¹˜ë¡œ URL ì²˜ë¦¬"""
        if not self.check_daily_limit():
            logger.info("ì¼ì¼ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        # ì•„ì§ ì œì¶œí•˜ì§€ ì•Šì€ URL í•„í„°ë§
        pending_urls = [
            url_info for url_info in urls
            if url_info['url'] not in self.log_data['urls']
            or self.log_data['urls'][url_info['url']]['status'] == 'error'
        ]

        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        pending_urls.sort(key=lambda x: float(x.get('priority_score', 0)), reverse=True)

        logger.info(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ê¸° URL: {len(pending_urls)}ê°œ")

        # ë°°ì¹˜ ì²˜ë¦¬
        processed = 0
        for url_info in pending_urls[:batch_size]:
            url = url_info['url']

            if not self.check_daily_limit():
                logger.info("âš ï¸  ì¼ì¼ ì œí•œ ë„ë‹¬. ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨")
                break

            logger.info(f"ğŸ”„ ì²˜ë¦¬ ì¤‘ ({processed + 1}/{min(batch_size, len(pending_urls))}): {url}")
            success = self.request_indexing(url)

            if success:
                processed += 1
                self.log_data['total_submitted'] += 1

            # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(1)

        # ë¡œê·¸ ì €ì¥
        self.log_data['last_run'] = datetime.now().isoformat()
        self.save_log()

        logger.info(f"\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {processed}ê°œ URL ì œì¶œ")
        logger.info(f"ğŸ“Š ì´ í†µê³„:")
        logger.info(f"   - ì´ ì œì¶œ: {self.log_data['total_submitted']}")
        logger.info(f"   - ì„±ê³µ: {self.log_data['success_count']}")
        logger.info(f"   - ì‹¤íŒ¨: {self.log_data['error_count']}")

    def generate_report(self) -> str:
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("Google Search Console ìƒ‰ì¸ ìš”ì²­ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        report.append("ğŸ“Š ì „ì²´ í†µê³„")
        report.append(f"  â€¢ ì´ ì œì¶œ URL: {self.log_data['total_submitted']}")
        report.append(f"  â€¢ ì„±ê³µ: {self.log_data['success_count']}")
        report.append(f"  â€¢ ì‹¤íŒ¨: {self.log_data['error_count']}")
        report.append(f"  â€¢ ì˜¤ëŠ˜ ì œì¶œ: {self.log_data.get('submitted_today', 0)}/{DAILY_LIMIT}")
        report.append("")

        # ìµœê·¼ ì œì¶œ URL
        report.append("ğŸ“‹ ìµœê·¼ ì œì¶œ URL (ìµœëŒ€ 10ê°œ)")
        urls_by_time = sorted(
            self.log_data['urls'].items(),
            key=lambda x: x[1]['timestamp'],
            reverse=True
        )

        for url, info in urls_by_time[:10]:
            status_icon = "âœ…" if info['status'] == 'submitted' else "âŒ"
            report.append(f"  {status_icon} {url}")
            report.append(f"     ì‹œê°: {info['timestamp']}")
            if info['status'] == 'error':
                report.append(f"     ì˜¤ë¥˜: {info.get('error', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
            report.append("")

        report.append("=" * 60)

        report_text = "\n".join(report)
        logger.info(f"\n{report_text}")

        # íŒŒì¼ë¡œ ì €ì¥
        with open('indexing_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)

        return report_text


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Google Search Console ìƒ‰ì¸ ìë™í™” ì‹œì‘\n")

    try:
        automation = IndexingAutomation()
        urls = automation.load_urls()

        if not urls:
            logger.error("ì²˜ë¦¬í•  URLì´ ì—†ìŠµë‹ˆë‹¤")
            return

        automation.process_batch(urls, BATCH_SIZE)
        automation.generate_report()

        logger.info("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
